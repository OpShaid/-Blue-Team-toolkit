"""
Static Malware Analyzer - Analyze files without execution.
"""

import hashlib
import struct
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass, field

from blueteam.core.logger import get_logger
from blueteam.core.utils import hash_file, entropy, get_file_type

logger = get_logger(__name__)


@dataclass
class PESection:
    """Represents a PE file section."""
    name: str
    virtual_address: int
    virtual_size: int
    raw_size: int
    characteristics: int
    entropy: float


@dataclass
class PEImport:
    """Represents a PE import."""
    dll: str
    functions: List[str]


@dataclass
class AnalysisResult:
    """Result of static analysis."""
    file_path: str
    file_name: str
    file_size: int
    file_type: str
    hashes: Dict[str, str]
    entropy: float
    is_packed: bool
    is_suspicious: bool
    risk_score: int
    indicators: List[str]
    strings_of_interest: List[str]
    pe_info: Optional[Dict[str, Any]] = None
    imports: List[PEImport] = field(default_factory=list)
    sections: List[PESection] = field(default_factory=list)
    yara_matches: List[str] = field(default_factory=list)
    analysis_time: Optional[datetime] = None


class StaticAnalyzer:
    """
    Perform static analysis on potentially malicious files.

    Features:
    - File type identification
    - Hash calculation (MD5, SHA1, SHA256)
    - Entropy analysis
    - PE file parsing
    - Import analysis
    - String extraction
    - Packer detection
    - Suspicious indicator detection
    """

    # Suspicious imports often used by malware
    SUSPICIOUS_IMPORTS = {
        "high": [
            "VirtualAlloc", "VirtualProtect", "WriteProcessMemory",
            "CreateRemoteThread", "NtUnmapViewOfSection", "ResumeThread",
            "SetThreadContext", "QueueUserAPC", "RtlCreateUserThread",
            "LoadLibraryA", "GetProcAddress", "CreateProcessA",
        ],
        "medium": [
            "CreateFileA", "CreateFileW", "WriteFile", "ReadFile",
            "RegCreateKeyA", "RegSetValueExA", "RegOpenKeyExA",
            "InternetOpenA", "InternetConnectA", "HttpSendRequestA",
            "URLDownloadToFileA", "WinExec", "ShellExecuteA",
            "CreateServiceA", "OpenServiceA", "StartServiceA",
        ],
        "low": [
            "GetModuleHandleA", "GetSystemDirectoryA", "GetWindowsDirectoryA",
            "GetTempPathA", "CopyFileA", "DeleteFileA", "MoveFileA",
        ],
    }

    # Known packer signatures
    PACKER_SIGNATURES = {
        "UPX": b"UPX!",
        "UPX0": b"UPX0",
        "UPX1": b"UPX1",
        "ASPack": b"ASPack",
        "PECompact": b"PEC2",
        "Themida": b"Themida",
        "VMProtect": b".vmp",
        "Obsidium": b"Obsidium",
    }

    # Suspicious strings patterns
    SUSPICIOUS_PATTERNS = [
        (r"(https?://[\w\-\.]+\.[\w]{2,}[^\s]*)", "url"),
        (r"\b([A-Za-z]:\\[^\s]+\.exe)\b", "exe_path"),
        (r"\bHKEY_[A-Z_]+\\[^\s]+", "registry"),
        (r"powershell.*-e[ncodema]*\s+[A-Za-z0-9+/=]+", "encoded_powershell"),
        (r"cmd\.exe\s*/c", "cmd_execution"),
        (r"\\\\[\w\.\-]+\\[\w$]+", "network_share"),
        (r"\b(?:\d{1,3}\.){3}\d{1,3}\b", "ipv4"),
        (r"[A-Za-z0-9+/]{50,}={0,2}", "base64"),
    ]

    def __init__(self, yara_rules_path: str = None):
        self.yara_rules_path = yara_rules_path
        self.results: List[AnalysisResult] = []

    def analyze(self, file_path: str) -> AnalysisResult:
        """Perform complete static analysis on a file."""
        path = Path(file_path)

        if not path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        logger.info(f"Analyzing file: {path.name}")

        # Basic file info
        result = AnalysisResult(
            file_path=str(path.absolute()),
            file_name=path.name,
            file_size=path.stat().st_size,
            file_type=get_file_type(path).get("type", "Unknown"),
            hashes=hash_file(path),
            entropy=0,
            is_packed=False,
            is_suspicious=False,
            risk_score=0,
            indicators=[],
            strings_of_interest=[],
            analysis_time=datetime.now(),
        )

        # Read file content
        with open(path, "rb") as f:
            content = f.read()

        # Calculate entropy
        result.entropy = entropy(content)
        if result.entropy > 7.0:
            result.indicators.append(f"High entropy ({result.entropy:.2f}) - possibly packed/encrypted")
            result.risk_score += 20

        # Check for packers
        packer = self._detect_packer(content)
        if packer:
            result.is_packed = True
            result.indicators.append(f"Packed with {packer}")
            result.risk_score += 15

        # PE Analysis
        if content[:2] == b"MZ":
            self._analyze_pe(content, result)

        # Extract suspicious strings
        result.strings_of_interest = self._extract_suspicious_strings(content)
        if result.strings_of_interest:
            result.risk_score += min(len(result.strings_of_interest) * 2, 20)

        # YARA scanning
        if self.yara_rules_path:
            result.yara_matches = self._scan_yara(content)
            if result.yara_matches:
                result.risk_score += len(result.yara_matches) * 10

        # Determine if suspicious
        result.is_suspicious = result.risk_score >= 30
        result.risk_score = min(result.risk_score, 100)

        self.results.append(result)
        return result

    def _detect_packer(self, content: bytes) -> Optional[str]:
        """Detect known packers."""
        for packer_name, signature in self.PACKER_SIGNATURES.items():
            if signature in content:
                return packer_name
        return None

    def _analyze_pe(self, content: bytes, result: AnalysisResult):
        """Analyze PE file structure."""
        try:
            # DOS Header
            if len(content) < 64:
                return

            e_lfanew = struct.unpack("<I", content[60:64])[0]

            if e_lfanew + 24 > len(content):
                return

            # PE Signature
            if content[e_lfanew:e_lfanew + 4] != b"PE\x00\x00":
                return

            # COFF Header
            coff_offset = e_lfanew + 4
            machine, num_sections, timestamp, _, _, opt_header_size, characteristics = struct.unpack(
                "<HHIIIHH", content[coff_offset:coff_offset + 20]
            )

            # Compile timestamp
            compile_time = datetime.fromtimestamp(timestamp) if timestamp > 0 else None

            result.pe_info = {
                "machine": self._get_machine_type(machine),
                "num_sections": num_sections,
                "compile_time": compile_time.isoformat() if compile_time else None,
                "characteristics": characteristics,
                "is_dll": bool(characteristics & 0x2000),
                "is_executable": bool(characteristics & 0x0002),
            }

            # Optional Header
            opt_offset = coff_offset + 20
            if opt_header_size >= 2:
                magic = struct.unpack("<H", content[opt_offset:opt_offset + 2])[0]
                result.pe_info["is_64bit"] = magic == 0x20b

            # Parse sections
            section_offset = opt_offset + opt_header_size
            for i in range(num_sections):
                if section_offset + 40 > len(content):
                    break

                section_data = content[section_offset:section_offset + 40]
                name = section_data[:8].rstrip(b'\x00').decode('ascii', errors='ignore')
                virtual_size, virtual_addr, raw_size, raw_addr = struct.unpack(
                    "<IIII", section_data[8:24]
                )
                chars = struct.unpack("<I", section_data[36:40])[0]

                # Calculate section entropy
                if raw_size > 0 and raw_addr + raw_size <= len(content):
                    section_content = content[raw_addr:raw_addr + raw_size]
                    section_entropy = entropy(section_content)
                else:
                    section_entropy = 0

                section = PESection(
                    name=name,
                    virtual_address=virtual_addr,
                    virtual_size=virtual_size,
                    raw_size=raw_size,
                    characteristics=chars,
                    entropy=section_entropy,
                )
                result.sections.append(section)

                # Check for suspicious sections
                if section_entropy > 7.5:
                    result.indicators.append(f"High entropy section: {name} ({section_entropy:.2f})")
                    result.risk_score += 10

                if name not in [".text", ".data", ".rdata", ".rsrc", ".reloc", ".bss"]:
                    result.indicators.append(f"Unusual section name: {name}")
                    result.risk_score += 5

                section_offset += 40

            # Parse imports (simplified)
            self._parse_imports(content, result)

        except Exception as e:
            logger.debug(f"PE parsing error: {e}")

    def _get_machine_type(self, machine: int) -> str:
        """Get machine type name."""
        machines = {
            0x014c: "i386",
            0x8664: "AMD64",
            0x01c0: "ARM",
            0xaa64: "ARM64",
        }
        return machines.get(machine, f"Unknown ({hex(machine)})")

    def _parse_imports(self, content: bytes, result: AnalysisResult):
        """Parse PE imports (simplified extraction)."""
        # This is a simplified import extraction using string search
        # A full implementation would parse the import directory table

        suspicious_found = []

        for level, imports in self.SUSPICIOUS_IMPORTS.items():
            for imp in imports:
                if imp.encode() in content:
                    suspicious_found.append((imp, level))

        if suspicious_found:
            for imp, level in suspicious_found:
                if level == "high":
                    result.indicators.append(f"Suspicious import: {imp}")
                    result.risk_score += 10
                elif level == "medium":
                    result.risk_score += 5

            # Group by detected DLLs
            dll_patterns = [
                b"kernel32.dll", b"ntdll.dll", b"user32.dll",
                b"advapi32.dll", b"ws2_32.dll", b"wininet.dll",
            ]

            for dll in dll_patterns:
                if dll in content.lower():
                    result.imports.append(PEImport(
                        dll=dll.decode(),
                        functions=[imp for imp, _ in suspicious_found if imp.encode() in content]
                    ))

    def _extract_suspicious_strings(self, content: bytes) -> List[str]:
        """Extract suspicious strings from file."""
        strings = []

        try:
            text = content.decode('utf-8', errors='ignore')
        except Exception:
            text = content.decode('latin-1', errors='ignore')

        for pattern, pattern_type in self.SUSPICIOUS_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches[:10]:  # Limit matches per pattern
                if isinstance(match, tuple):
                    match = match[0]
                if len(match) > 5 and len(match) < 500:
                    strings.append(f"[{pattern_type}] {match}")

        return strings[:50]  # Limit total strings

    def _scan_yara(self, content: bytes) -> List[str]:
        """Scan with YARA rules."""
        matches = []

        try:
            import yara
            rules = yara.compile(filepath=self.yara_rules_path)
            yara_matches = rules.match(data=content)
            matches = [str(m) for m in yara_matches]
        except ImportError:
            logger.debug("YARA not installed")
        except Exception as e:
            logger.debug(f"YARA scan error: {e}")

        return matches

    def analyze_batch(self, file_paths: List[str]) -> List[AnalysisResult]:
        """Analyze multiple files."""
        results = []
        for path in file_paths:
            try:
                result = self.analyze(path)
                results.append(result)
            except Exception as e:
                logger.error(f"Failed to analyze {path}: {e}")
        return results

    def get_report(self, result: AnalysisResult) -> str:
        """Generate analysis report."""
        report = f"""
# Static Analysis Report

## File Information
- **Name:** {result.file_name}
- **Path:** {result.file_path}
- **Size:** {result.file_size:,} bytes
- **Type:** {result.file_type}

## Hashes
- **MD5:** {result.hashes.get('md5', 'N/A')}
- **SHA1:** {result.hashes.get('sha1', 'N/A')}
- **SHA256:** {result.hashes.get('sha256', 'N/A')}

## Analysis Results
- **Entropy:** {result.entropy:.2f}
- **Is Packed:** {result.is_packed}
- **Is Suspicious:** {result.is_suspicious}
- **Risk Score:** {result.risk_score}/100

## Indicators
"""
        for indicator in result.indicators:
            report += f"- {indicator}\n"

        if result.pe_info:
            report += f"""
## PE Information
- **Machine:** {result.pe_info.get('machine')}
- **Sections:** {result.pe_info.get('num_sections')}
- **Compile Time:** {result.pe_info.get('compile_time')}
- **Is DLL:** {result.pe_info.get('is_dll')}
- **Is 64-bit:** {result.pe_info.get('is_64bit')}
"""

        if result.sections:
            report += "\n## Sections\n"
            report += "| Name | Virtual Size | Raw Size | Entropy |\n"
            report += "|------|-------------|----------|----------|\n"
            for section in result.sections:
                report += f"| {section.name} | {section.virtual_size} | {section.raw_size} | {section.entropy:.2f} |\n"

        if result.strings_of_interest:
            report += "\n## Suspicious Strings\n"
            for s in result.strings_of_interest[:20]:
                report += f"- `{s}`\n"

        if result.yara_matches:
            report += "\n## YARA Matches\n"
            for match in result.yara_matches:
                report += f"- {match}\n"

        return report

    def export_results(self, output_file: str):
        """Export all results to JSON."""
        import json

        data = []
        for result in self.results:
            data.append({
                "file_name": result.file_name,
                "file_path": result.file_path,
                "file_size": result.file_size,
                "file_type": result.file_type,
                "hashes": result.hashes,
                "entropy": result.entropy,
                "is_packed": result.is_packed,
                "is_suspicious": result.is_suspicious,
                "risk_score": result.risk_score,
                "indicators": result.indicators,
                "strings_of_interest": result.strings_of_interest[:20],
                "pe_info": result.pe_info,
                "yara_matches": result.yara_matches,
                "analysis_time": result.analysis_time.isoformat() if result.analysis_time else None,
            })

        with open(output_file, "w") as f:
            json.dump(data, f, indent=2)

        logger.info(f"Exported {len(data)} results to {output_file}")
